\section{Introduction}
\exampleFig

Uncertainty is an inescapable component of collecting, analyzing, and presenting data. A common goal in the communication of uncertainty is promoting \emph{uncertainty-aware decisions}: the audience should be aware of the risks and rewards of certain decisions, modulate their confidence in their conclusions, and perhaps refrain from making a decision at all if there is too much uncertainty.  A way that designers can contribute to this goal is by ensuring that uncertainty information is \emph{well-integrated} with the rest of the data. That is, it should be difficult to discount or ignore the uncertainty in a dataset.

Uncertainty is an inescapable component of collecting, presenting, and using data. A common goal in the communication of uncertainty is promoting \emph{uncertainty-aware decisions}: the audience should be aware of the risks and rewards of certain decisions, modulate their confidence in their conclusions, and perhaps refrain from making a decision at all if there is too much uncertainty.  A way that designers can contribute to this goal is by ensuring that uncertainty information is \emph{well-integrated} with the rest of the data. That is, it should be difficult to discount or ignore the uncertainty in a dataset.

Simultaneous presentation of uncertainty and value necessitates the construction of a bivariate map\,---\,a relation, in terms of visual variables, between 2-tuples $(\text{value}, \text{uncertainty})$ and mark properties. Due to the interference and interplay between different visual variables, bivariate maps may suffer from limited discriminability.

In this paper, we contribute \textbf{Value-Suppressing Uncertainty Palettes} (VSUPs) for integrating data and uncertainty information in visualizations.
VSUPs intentionally alias together data values with high uncertainty, affording greater discriminability as uncertainty decreases. Traditional bivariate maps might be thought of as a 2D square, with differing outputs for each combination of value and uncertainty. In contrast, VSUPs can be conceptualized as arcs: as uncertainty increases, values are mapped to smaller and smaller sets of outputs, culminating in a singularity where all inputs are mapped to an identical, highly uncertain mark regardless of data value. \figref{fig:example} shows examples of both a traditional bivariate map and a VSUP.

We describe the motivations for VSUPs, provide examples of their utility for decision-making under uncertainty, and assess VSUPs in a crowdsourced experiment. Our results indicate that VSUPs create close integration between uncertainty and data, promoting rapid but cautious decision-making.

\section{Related Work}

% What this section needs to do:
% State of the art in uncertainty vis, with heavy emphasis on MacEachren & co.
% Bivariate maps are hard!
% Binning colormaps is de rigueur
% The variables we'd want to use for uncertainty (like value or alpha or size) mess with color discriminability

Despite the acknowledged importance of uncertainty in understanding data, explicit representations of uncertainty are often missing from visualizations~\cite{boukhelifa2009uncertainty}. This is partially due to the complexity of uncertainty as a concept. In typologies from Thomson \ea~\cite{thomson2005typology} and Buttenfield \& Beard~\cite{buttenfield1994graphical}, the authors note that many, occasionally contradictory, concepts can fall under the category of ``uncertainty,'' including data quality, sampling error, credibility, and provenance.

There are many methods for visualizing uncertainty. MacEachren \ea~\cite{maceachren1992visualizing} present a theoretical survey of uncertainty visualization. Relevant to our domain of heatmaps and thematic maps \textbf{JH: where did this come from? There is no mention of focusing on this domain in the intro. The whole carto/GIS stuff seems to come out of left field.} is work in GIS and cartography; for a survey of uncertainty visualization techniques in GIS, see Kinkeldey \ea~\cite{kinkeldey2014assess,kinkeldey2017evaluating}. Yet, despite a wide variety of methods, to our knowledge, there are no established standards for visualizing uncertainty in heatmaps.

Reviewing the state of the art in uncertainty visualization in the field, Greithe \ea~\cite{griethe2006visualization} and Brodlie \ea~\cite{brodlie2012review} present lists of potential techniques for conveying uncertainty in concert with data. Some of these options (interaction, animation, and sonification) are not applicable for static charts; even for dynamic charts, many users do not interact with charts in sufficient detail to recover uncertainty information~\cite{nyt2016}. While we acknowledge the potential utility of these techniques in uncertainty visualization (for instance, the animation used to convey sampling error in Hullman \ea's Hypothetical Outcome Plots~\cite{hullman2015hypothetical}), we limit the scope of our discussion to techniques which center on static charts.

\subsection{Visual Variables for Uncertainty}

% 1d uvis: olston2002visualizing, wickham201140, neyman1937outline

One challenge of uncertainty visualization is that the decision to explicitly encode uncertainty increases the dimensionality of the data, and so requires the use of (at least one) additional visual channel~\cite{brodlie2012review}. Uncertainty therefore inherently increases the visual complexity of a visualization. When the data are already complex to convey, and many of the more common or accurate visual variables are in use, allocating an additional dimension is non-trivial. As the number of dimensions increases, finding visual variables that are both perceptually accurate (in either estimation of quantity or discrimination of category) as well as perceptually separable from all the other encoding channels, becomes increasingly difficult. 

A further hurdle is that not all visual variables are well-suited for conveying uncertainty. MacEachren \ea~\cite{maceachren2012visual} evaluate a number of visual variables with respect to their \emph{semiotic fit} for representing uncertainty. They observe that certain visual variables such as blurriness and transparency seem to have a more intuitive connection to uncertainty than other variables such as shape or hue. Unfortunately, Boukhelifa \ea~\cite{boukhelifa2012evaluating} find that many visual variables habitually used for conveying uncertainty, such as blur and value, are also difficult to estimate. This results in a preference/performance gap where designers must choose between encoding uncertainty in a way that is intuitive but error prone, or use higher fidelity channels that may not intuitively convey uncertainty.

\subsection{Color Maps}

For spatial visualizations like choropleth maps, heatmaps, and treemaps, visual channels such as position and length are reserved for data variables other than value (such as geographic location or relative size). In these situations, data value is often encoded using color. Colors in univariate quantitative color maps should be sufficiently far apart as to be perceptually distinguishable~\cite{ware1988color}, and vary in lightness as well as hue to afford an implicit ordering of value~\cite{borland2007rainbow,rogowitz2001blair}. Different choices of color maps can highlight different features of the data, and should be chosen with care~\cite{rogowitz1996not}.

A recurring design consideration when constructing color maps is whether to quantize data into a discrete set of colors, or to encode data using a continuous mapping. While continuous color maps afford greater fidelity in presenting values~\cite{muller1979perception}, non-linearity in human color perception introduces errors in extracting numeric values from continuous colors~\cite{borland2007rainbow}. Quantizing a color map is therefore an exercise in balancing \emph{perceptual} error and \emph{quantization} error~\cite{dobson1973choropleth}. Discrete maps offer finer control over this balance, which can result in better performance in tasks involving heatmaps~\cite{padilla2017evaluating}. As such, we limit our discussion to color maps with discrete sets of colors.

\subsection{Bivariate Maps}

While uncertainty can be visualized entirely separately from the rest of the data (for instance, in a juxtaposed chart), this runs the risk that uncertainty information becomes \emph{ignorable}~\cite{moritz2017trust}. It also complicates analysis, as interpreting the value and uncertainty of a given data point requires consulting two separate charts. To integrate uncertainty information into an analysis, we focus on the construction of \emph{bivariate maps}, where value and uncertainty information is displayed simultaneously in a spatially co-located manner.

Bivariate maps can, in principle, be constructed from the combination of any two visual variables (such as shape and color, or size and texture). For example Ware's ``textons''~\cite{ware2009quantitative} overlay glyphs on top of regions to simultaneously encode two quantitative values. However, in this work, our focus is on heatmaps and other sorts of thematic maps. In visualizations like heatmaps, common visual channels such as position and size are already in use to encode other information (such as spatial location and extent), and color is reserved for value \textbf{JH: this is a bit redundant with the previous subsection on color maps. It feels a bit artificial having these two sections separate. Color is a subset of bivariate map options... and we start talking about it again in the next paragraph.}. Bivariate maps in these settings therefore typically rely on a visual channel related to color for encoding a secondary variable, such as opacity (alpha)~\cite{roth2010value} or pixel noise~\cite{lucchesi2017visualizing}.

Due to the perceptual integrality of visual channels~\cite{garner1970integrality, ware2012information}, \textbf{JH: We mention integrality but never give a brief definition or explanation. I wouldn't assume that all CHI readers are familiar with this.} the construction of color-based bivariate maps is difficult. Viewers need to be able to independently assess the variables of interest \textbf{JH: They do? I thought the whole point of VSUPs is to confound indepenent assessment of value!}, but the resulting bivariate map should also be expressive and easy to interpret. Bivariate maps with these constraints are often limited to relatively few categories (say, a 4x4 matrix as in \figref{fig:example}) \cite{robertson1986generation,trumbo1981theory}. For VSUPs, we adapt an insight from Dunn~\cite{dunn1989dynamic} that these limited categories ought to be assigned with regards to their importance to analysis tasks, rather than uniformly.

In general, the quality of bivariate color maps is a multivariate measure involving consideration of not just the component color channels, but also the interpolation scheme and the color of the surround~\cite{bernard2015survey}. However, even simple bivariate maps can be difficult for a general audience to interpret~\cite{wainer1980empirical} \textbf{JH: It seems there is a lot ``hiding'' behind this citation. In what ways is it hard? In what ways do people fail? Any more flavor here, even if very brief? And might this motivate something akin to VSUPs?}.

\textbf{JH: This would be a natural place to put some connective prose that transitions to VSUPs...}

\section{Value-Suppressing Uncertainty Palettes}

%TODO need some example figure here, showing tree structure perhaps?
%\flowFig

Value-Suppressing Uncertainty Palettes (VSUPs) are a technique for creating bivariate maps of data \emph{value} and \emph{uncertainty}. VSUPs make two central assumptions about bivariate maps:

\begin{enumerate}
	\item There is a limited \emph{budget} for perceptual discriminability.
	\item Differences among \emph{certain} data are more germane than differences among \emph{uncertain} data.
\end{enumerate}

In some cases, these assumptions are violated. For instance, an analyst might be interested in ``long tail risks'' or other ``black swan events,'' where the impact of a value, no matter how uncertain, must be considered and planned for~\cite{taleb2011black}. Other analysis tasks (such as filtering out outliers), require increased, rather than decreased, discriminability when uncertainty is high. However, for many information fusion tasks, the assumption is that uncertainty is related to data quality, or the uncertainty of data values~\cite{riveiro2007evaluation}.

However, if both of these assumptions hold, then it follows that the designer of a bivariate map should allocate more mark types to certain values, and fewer mark types to uncertain values. VSUPs codify this decision by \emph{reducing the number of mark categories for representing value as uncertainty increases.} This means that data encoded using a VSUP will make visible only the largest of differences in uncertain data, but highlight comparatively small differences in value when uncertainty is low. VSUPs therefore act as both a filtering mechanism (in that values with too much uncertainty are mapped to the same glyph), as well as an implicit test of effect size (in that smaller and smaller changes in value are visible as uncertainty decreases). This strategy of dampening low quality or highly uncertain values in maps in order to focus on more informative regions has measurable benefits, including the removal of statistically spurious visual patterns, and the highlighting of regions of interest~\cite{correll2017surprise}.

VSUPs rely on an underlying \textbf{quantization tree} that governs how values are discretized. Maximally uncertain values are mapped to a singular ``root'' node. As uncertainty decreases, the tree branches into leaves, which evenly divide the data domain. The data value of a parent is the midpoint of all of its children. These leaves can then branch again, up to a designer-specified stopping point. The layers quantize the uncertainty domain, and sibling nodes quantize the data domain. \figref{fig:example} shows a VSUP with a tree with a branching factor of 2 and 4 layers, resulting in an output range with 15 distinct colors.

Correll \ea~\cite{correll2015layercake,correll2011visualizing} use a precursor of VSUPs in their LayerCake genomics visualization tool, where marks representing genomic data with increasing uncertainty are mapped to a smaller and smaller set of increasingly grey colors, creating the effect of uncertain values retreating into a ``confidence fog'' while highly certain values remain prominent. Other bivariate visualizations implicitly alias together uncertain values. For instance, if uncertainty is encoded by transparency, a maximally uncertain glyph may be entirely transparent, and so impossible to distinguish from any other maximally uncertain glyph. Other channels with a semiotic connection to uncertainty, such as saturation, value, blur, or size, also have deleterious effects on the disambiguation of colors and shapes. In both cases, the property of aliasing is \emph{ad hoc}, and places no guarantees on the discriminability of colors. The binning and degradation approach of VSUPs makes the choice to alias values explicit to both the designer and the viewer, and results in a bivariate mapping with known perceptual properties.

\textbf{JH: The precending paragraph might be better placed in related work!}

JavaScript code for generating VSUPs for use in D3 charts is available at \url{URL-REMOVED-FOR-REVIEW}.

\subsection{Design Considerations}

%\performanceFig

There are multiple choices that designers must make before creating a VSUP. Specifically, they must choose (1) which visual channels map to value and uncertainty and (2) the structure of the underlying VSUP tree.

Often, the graphical conventions of a chart reserve variables such as position and area for other uses, and determine the visual channel to be used for value. In heatmaps, for instance, value is regularly encoded using color, while vertical and horizontal position are reserved for location. \textbf{JH: This is the third time we've pointed this out...} In this paper, we primarily deal with the case where value is encoded using color. While there are some similar conventions for representing uncertainty, generally designers have more freedom in choosing a visual channel for uncertainty~\cite{maceachren1992visualizing}. \textbf{This feels like we are revisiting introductory and related work material again!} We recommend channels with both a strong semiotic connection to uncertainty as well as a relatively large number of perceptually distinguishable levels. Here, we focus on value/saturation (or, equivalently, transparency against a white background). Correll \& Gleicher~\cite{correll2013error} provide evidence that even audiences without statistical training can successfully interpret uncertainty information encoded in these channels.

\textbf{We should say something about the choice of value color map. Generally speaking, single-hue luminance ramps must be used with caution if they ramp too strongly towards white. This will be confused with manipulations of opacity or saturation. Viridis might be attractive due to its avoidance of desaturated colors.}

%TODO Add in discussion of tree structures, including figure showing that VSUPs have colors that are further apart than similar 2d bivariate maps. Also worth discussing discretization issues here. Make new performance fig.


\subsection{Examples}

We present a set of examples showing how VSUPs can be applied to realworld datasets. The decision of VSUPs to allocate color bins asymmetrically amongst value and uncertainty regions affords greater discriminability in regions of the chart that warrant closer scrutiny, while discouraging exploration of regions with noisy or unreliable signals.


\subsubsection{Air Travel Delay}

\airlineFig

Using a public data set from the U.S. Bureau of Transportation Statistics~\cite{bts}, we created a heatmap showing average flight delay for different times of the day and days of the week for U.S. carriers in January 2017. We use standard error $\left(\sigma / \sqrt{n}\right)$ as our measure of uncertainty. We created two alternate heatmaps of this delay information. \figref{fig:airline2d} shows a traditional bivariate map, whereas \figref{fig:airlineVsum} shows a VSUP generated under the same constraints.

Both maps illustrate a similar temporal trend: flight delays are shorter towards the beginning of the day, increasing on average over time. The traditional bivariate map affords only a coarse examination of this trend: two visible ``blocks'' of color, early in the day, and later in the afternoon. The VSUP, in contrast, has sufficient color resolution to show a quasi-linear trend. Uncertainty is high only for a few bins for which there are small number of flights, either on the weekends or late at night. The VSUP does not obscure any significant trends in the high uncertainty data, despite having fewer color categories overall.

\subsubsection{Viral Mutation}
%TODO update this figure
\viralFig

Using a dataset from a group of virologists interested in the population dynamics of viral mutation~\cite{o2012conditional}, we visualized the variability in viral genomes across 25 populations of the simian immunodeficiency virus (SIV). Next generation sequencing (NGS) allows biologists to collect and analyze large amounts of genomics data. However, these techniques often create data quality issues, as they must align large numbers of potentially ambiguous ``reads'' of relatively small numbers of base pairs. In the dataset here, it is important not only to identify hotspots in the viral genome (locations with high rates of mutation or variability), but to not be distracted by false positives. The traditional bivariate map (\figref{fig:viral2d}) affords the analysis of only a few, large hot spots. Extending the ``confidence fog'' metaphor used in viral genome browsers like LayerCake~\cite{correll2015layercake}, VSUPs (\figref{fig:viralVsum}) enable biologists to explore interesting patterns while not being distracted by noise. Smaller scale ``hot spots'' that are mapped to the same magenta color in the traditional map are visible as bright orange lines. Pink hot spots towards the end of the genome (around reference nucleotide $9,500$) that could be plausibly considered potential regions of interest in the 2D map, are encoded as highly uncertain and of ambiguous value in the VSUPs. In fact, these regions have systematically low coverage; the lack of reliable data for these regions should discourage analysts from giving their apparent pattern equal consideration from the  stronger signals earlier in the genome.

\textbf{A few thoughts about this section based on the current figures. (1) Fig. 3 in particular has two graphics that look pretty similar. Is there anything we should do to draw attention to the most salient differences between conditions? At minimum, we have the text, but we could consider annotations or even a third vis that is a heatmap of differences between charts. (2) A third example using a different visual form might be useful. For example, a node-link network with value + uncertainty associated with each node.}

\section{Evaluation}

\conditionFig

We performed two crowdsourced experiments on Amazon's Mechanical Turk to evaluate the effectiveness of VSUPs for integrating uncertainty and value information in visualizations. This focus on integration meant that we limited our experimental tasks to scenarios where the participants needed to consider both value and uncertainty before making a decision. The experiments consisted of two separate tasks:

\begin{enumerate}
	\item An experiment with an \textbf{identification} task, where we gave participants charts with value and uncertainty information, and asked them to locate specific regions. E.g., ``click on the region of the chart with a value of $0.1$ and an uncertainty of $0.2$.''
	\item An experiment with a \textbf{prediction} task, inspired by the game Battleship, where we gave participants charts with both \emph{forecast} and \emph{forecast uncertainty} information, and asked the participants to place tokens on the board in order to optimize expected value. E.g., ``place your $5$ ships on safe locations on the board.''
\end{enumerate}

We limited our population to Turkers from the United States, with a prior task approval rate of at least 90\%. As the experimental tasks required multi-hue color perception, we presented participants with a set of Ishihara plates~\cite{hardy1945tests} as a pretest, and excluded participants who either misidentified the values in the plates, or who self-reported as having a color vision deficiency (CVD) in the post-test. Based on piloting, we paid participants \$2 dollars for participation, for a target rate of \$8/hour. After completion of the main tasks, we solicited demographic information, including a risk aversion assay from Mandrik \& Bao~\cite{mandrik2005exploring}. We recruited a total of $48$ participants for this experiment ($24$ for each experiment): $N$ female, $N$ male, and $N$ who declined to state ($M_{\text{age}}$= $N$, $SD_{\text{age}}$ = $N$). Participants from the first experiment were excluded from participating in the second.

Rather than standard means, Cleveland \& McGill~\cite{cleveland1984graphical} use 25\% trimmed means to report effect sizes in graphical perception tasks. Trimmed means remove the first and last quartiles and then compute the mean from the remaining points. This filtering is important for graphical perception tasks (especially crowdsourced tasks) where error distributions may be long-tailed~\cite{heer2010crowdsourcing}. Trimmed means violate the sampling assumptions required for many null-hypothesis significance tests. Thus, while we report effect sizes in terms of trimmed means, we perform inferential statistical tests on the standard means. Asterisks in our charts denote significant differences, as reported by a post-hoc Tukey's test of Honest Significant Difference (HSD).

Experimental materials, including data tables and stimuli generation code, are available at \url{URL-REMOVED-FOR-REVIEW}.


\subsection{Identification Experiment}

We wished to determine what effects, if any, our various design considerations had on performance at a basic analysis task: using a bivariate legend to interpret a heatmap. There are a number of design decisions to be made even after one has determined which two visual channels will be used to encode value and uncertainty.
In particular:
\begin{enumerate}
	\item Whether or not to \emph{juxtapose} or \emph{superimpose} the data and uncertainty maps.
	\item Whether or not to \emph{discretize} each visual channel, or encode values \emph{continuously}.
	\item Whether or not to use a \emph{square} legend, which gives equal area to each bin, or a \emph{wedge} legend, which reduces in size as uncertainty increases.
	\item If discretizing, whether to employ a \emph{VSUP} or a \emph{standard} quantization scheme.
\end{enumerate}

For the identification task, we gave participants a $5$x$5$ heatmap, and asked them to click on a region of the heatmap that encodes a particular value,uncertainty pair. To support comparability across conditions, we chose a set of target pairs such that each target mapped to a unique color across all encoding types. This resulted in (4+4+2+1)=11 valid targets. For each trial, these 11 targets were placed randomly in a heatmap; the remaining 13 cells of the heatmap were distractors randomly sampled from this target list.

We selected this task both as a way of assessing the ability of people to successfully encode and decode values using a bivariate map, and to provide training for the prediction task (since we re-used the bivariate maps in that task). We measured performance both in terms of accuracy (did the participant select the correct point) as well as response time.

Each of the 24 participants saw the data encoded as each of 8 types of bivariate encoding and legend, with 8 replications, for a total of 64 stimuli. \figref{fig:conditions} shows these factor levels.

\subsubsection{Hypotheses}

We had two initial hypotheses, based on prior work on bivariate maps:
\begin{enumerate}
	\item Juxtaposed maps, by introducing a second search task to the identification task (searching for the proper values in two, rather than one map), would have poorer performance than the other conditions.
	\item Continuous maps required more specific and accurate color matching between value and color, and so would have poorer performance than the other conditions.
\end{enumerate}

We had no strong hypotheses about the shape of the legend (square vs. wedge), but, as the wedge shape makes more uncertain bins in the legend smaller (and thus potentially harder to disambiguate), we included it as a check against VSUPs which explicitly alias uncertain values. For this identification task, which did not involve reasoning with uncertainty, but simply matching colors to a color legend, we also did not have strong hypotheses about performance differences between VSUPs and traditional discrete bivariate maps.

\subsubsection{Results}


\subsection{Prediction Experiment}

\taskTwoFig

For the prediction task, we gave participants the rules of a game like Battleship. Greis \ea~\cite{greis2016decision} employ these game-like experimental tasks to assess how different visual designs communicate uncertainty information, which can be abstract or complex, to the general audience. In our task, the participant and a (fictional) adversary have to place tokens representing ships on a $5$x$5$ spatial grid, with the expectation that certain squares will be hit by missiles. Players have to place all their tokens before continuing. The objective is to minimize the number of your own ships that are hit. In our task, participants were given a map representing the predictions of missile strikes in each location on the grid. The \emph{value} component of the prediction was the likelihood of a missile strike in a particular region: the ship's \emph{safety} if placed on the square. The \emph{uncertainty} component was the confidence in this prediction. Other studies of uncertainty representation, such as in Cox et al.~\cite{cox2013visualizing}, have used "prediction + prediction uncertainty" stimuli to elicit differences in decision-making between visualizations of uncertainty.

Our stimuli were created by randomly sampling from values that fell within each of the $16$ bins of the $4$x$4$ 2D bivariate color map. This resulted in 16 samples. The remaining 9 samples were distractors, with low safety and high certainty.

We selected this task in order to promote risk-averse behavior. Tversky \& Kahneman~\cite{tversky1985framing} illustrate that framings in terms of gains or losses produce reliably different outcomes. In particular, there is greater perceived value in avoiding large losses as opposed to striving for a large gain~\cite{kahneman1979prospect}. 

The ideal strategy from a value-maximizing standpoint would be to place tokens on areas with the highest expected value, ignoring the uncertainty information. However, as with roulette and other similar games of chance, the variability in expected value is relevant when considering where to place bets~\cite{mlodinow2009drunkard}. A risky player would choose guesses with high value, regardless of the uncertainty of those points. A more conservative guesser might eschew high-risk, high-reward locations, resulting in a lower average value of guesses, but also lower uncertainty. We therefore measured the distribution of both \emph{value} and \emph{uncertainty} of the tokens placed by the participants.

Our results from the prior study indicate indicate that discrete, non-juxtaposed maps outperformed the other bivariate maps we selected, so we limited our study to only 4 types: square and arc bivariate maps. These in turn were either VSUPs or traditional bivariate maps, for a 2 (square or arc legend) x 2 (VSUP or standard) factorial design, with 6 replications, for a total of 24 stimuli for each of our 24 participants. \figref{fig:conditions} shows these factor levels. Prior to the main task, we included a short replication of the identification task from the prior experiment (with 12 stimuli) for training purposes.

\subsubsection{Hypotheses}

We had two hypotheses, stemming from our belief that VSUPs promote better \emph{integration} between uncertainty and value information, and also encourage \emph{caution} by highlighting the ambiguity or untrustworthiness introduced by uncertain data. In particular:
\begin{enumerate}
	\item Participants would choose targets with \textbf{lower uncertainty} in the prediction task when using a VSUP. 
	\item This would result in a tradeoff where they would also choose targets with \textbf{lower expected value}.
\end{enumerate}

As with the prior experiment, we had no strong hypotheses for square vs. wedge-shaped legends, but included both as a check against the potential implicit VSUP-like properties of wedge-shaped legends.

\subsubsection{Results} 


\section{Discussion}

\sizeFig

The results from our initial evaluation of VSUPs are promising, showing that even a general audience can make use of uncertainty information in a reasonable way. The way that uncertainty information is presented can have a measurable impact on decision-making. The VSUP strategy of only ``showing'' differences when uncertainty is sufficiently low is somewhat analogous to the inferential statistics such as effects tests. VSUPs might therefore be used to promote more caution in judgments, and lead analysts away from spurious signals in data.

Bivariate maps have an inherent limitation in that visual channels are often difficult to attend to separately or orthogonally. Bivariate maps are also complex to interpret. Systems which simultaneously display large amounts of value and uncertainty data to general audiences (such as Pangloss~\cite{moritz2017trust}) avoid them for precisely these reasons, relying instead on juxtaposed maps. Our results indicate that juxtaposition produces additional problems, forcing users that wish to integrate uncertainty and data to locate an item of interest, and then re-locate that same item in a visually distant map, which may have few visual landmarks in common. This additional step before information fusion is reflected in longer response times, and patterns of decision-making that are less mindful of risk. Given the deleterious effects of latency on data analysis~\cite{liu2014effects}, designers ought to carefully consider the tradeoffs between expressiveness of visual encoding, and usability.

\subsection{Limitations and Future Work}
% When should you use these things?
In general, VSUPs excel in scenarios where we wish to make firm guarantees about the discriminability of different marks in a chart, but must also represent, in as much detail as possible, two, potentially orthogonal variables of interest. VSUPs have improved data resolution over other bivariate maps by assuming that uncertain values ought to have less visual impact that highly certain values. VSUPs therefore act both as a kind of filtering device as well as a bivariate representation. This assumption holds in scenarios like the virology dataset (\figref{fig:viralVsum}), where the expectation is that highly uncertain data is unreliable or should otherwise be downweighted in analysis. If the analyst has a different interpretation of uncertainty, or wishes to quickly and orthogonally analyze the distributions of uncertainty and value, other strategies, such as juxtaposed maps, may be more appropriate. VSUPs are designed for the \emph{integration} of value and uncertainty. Designers should take care in considering when and how this integration is desirable.

Our experiments dealt with cases where both uncertainty and value were represented by color. The perceptual non-separability of color channels is well-known~\cite{garner1970integrality, ware2012information}, and so the concept of a limited ``budget'' of distinguishable marks easier to quantify and illustrate. In principle, a VSUP can be created for any combination of visual variables. All that is required is a perceptual model of the interaction between these two variables. Where these models exist, as with the interaction between size and color~\cite{stone2014engineering}, the creation of VSUPs is straightforward. Where these models do not exist, or where the perceptual interaction is too complex to efficiently model, experimental work remains to be done before VSUPs can be considered a feasible design strategy. We are currently experimenting with the creation of VSUPs using these other channels, as in \figref{fig:size}.

\subsection{Conclusion}

Often, uncertainty, data quality, or confidence are considered separate information from the data itself, relegated to tooltips or visually distant supplemental charts. We believe, in contrast, that uncertainty information ought to be directly integrated with charts of data itself. This integration introduces additional complexity in the design and presentation of data. Value-Suppressing Uncertainty Palettes represent one strategy for dealing with this complexity, by assigning marks properties in a way that supports the disambiguation of values in data where uncertainty is low, but suppresses these judgments when uncertainty is high. This decision of how to allocate visual variables promotes patterns of decision-making that make responsible use of uncertainty information, discouraging comparison of values in unreliable regions of the data, and promoting comparison in regions of high certainty.