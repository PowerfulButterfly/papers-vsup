\section{Introduction}
\exampleFig

Uncertainty is an inescapable component of collecting, analyzing, and presenting data. A common goal in the communication of uncertainty is promoting \emph{uncertainty-aware decisions}: the audience should be aware of the risks and rewards of certain decisions, modulate their confidence in their conclusions, and perhaps refrain from making a decision at all if there is too much uncertainty.  A way that designers can contribute to this goal is by ensuring that uncertainty information is \emph{well-integrated} with the rest of the data. That is, it should be difficult to discount or ignore the uncertainty in a dataset.

Simultaneous presentation of uncertainty and value necessitates the construction of a bivariate map\,---\,a relation, in terms of visual variables, between 2-tuples $(\text{value}, \text{uncertainty})$ and mark properties. Due to the interference and interplay between different visual variables, bivariate maps may suffer from limited discriminability.

In this paper, we contribute \textbf{Value-Suppressing Uncertainty Palettes} (VSUPs) for integrating data and uncertainty information in visualizations.
VSUPs intentionally alias together data values with high uncertainty, affording greater discriminability as uncertainty decreases. Traditional bivariate maps might be thought of as a 2D square, with differing outputs for each combination of value and uncertainty. In contrast, VSUPs can be conceptualized as arcs: as uncertainty increases, values are mapped to smaller and smaller sets of outputs, culminating in a singularity where all inputs are mapped to an identical, highly uncertain mark regardless of data value. \figref{fig:example} shows examples of both a traditional bivariate map and a VSUP.

We describe the motivations for VSUPs, provide examples of their utility for decision-making under uncertainty, and assess VSUPs in a crowdsourced experiment. Our results indicate that VSUPs create close integration between uncertainty and data, resulting in better performance at data tasks with uncertainty components.

\section{Related Work}

% What this section needs to do:
% State of the art in uncertainty vis, with heavy emphasis on MacEachren & co.
% Bivariate maps are hard!
% Binning colormaps is de rigueur
% The variables we'd want to use for uncertainty (like value or alpha or size) mess with color discriminability

Despite the acknowledged importance of uncertainty in understanding data, explicit representations of uncertainty are often missing from visualizations~\cite{boukhelifa2009uncertainty}. This is partially due to the complexity of uncertainty as a concept. In typologies from Thomson \ea~\cite{thomson2005typology} and Buttenfield \& Beard~\cite{buttenfield1994graphical}, the authors note that many, occasionally contradictory, concepts can fall under the category of ``uncertainty,'' including data quality, sampling error, credibility, and provenance.

%There are many methods for visualizing uncertainty. MacEachren \ea~\cite{maceachren1992visualizing} present a theoretical survey of uncertainty visualization. Relevant to our domain of heatmaps and thematic maps \textbf{JH: where did this come from? There is no mention of focusing on this domain in the intro. The whole carto/GIS stuff seems to come out of left field.} is work in GIS and cartography; for a survey of uncertainty visualization techniques in GIS, see Kinkeldey \ea~\cite{kinkeldey2014assess,kinkeldey2017evaluating}. Yet, despite a wide variety of methods, to our knowledge, there are no established standards for visualizing uncertainty in heatmaps.

Reviewing the state of the art in uncertainty visualization in the field, Greithe \ea~\cite{griethe2006visualization} and Brodlie \ea~\cite{brodlie2012review} present lists of potential techniques for conveying uncertainty in concert with data. Some of these options (interaction, animation, and sonification) are not applicable for static charts; even for dynamic charts, many users do not interact with charts in sufficient detail to recover uncertainty information~\cite{nyt2016}. While we acknowledge the potential utility of these techniques in uncertainty visualization (for instance, the animation used to convey sampling error in Hullman \ea's Hypothetical Outcome Plots~\cite{hullman2015hypothetical}), we limit the scope of our discussion to techniques which center on static charts.

\subsection{Visual Variables for Uncertainty}

% 1d uvis: olston2002visualizing, wickham201140, neyman1937outline

One challenge of uncertainty visualization is that the decision to explicitly encode uncertainty increases the dimensionality of the data, and so requires the use of (at least one) additional visual channel~\cite{brodlie2012review}. Uncertainty therefore inherently increases the visual complexity of a visualization. When the data are already complex to convey, and many of the more common or accurate visual variables are in use, allocating an additional dimension is non-trivial. As the number of dimensions increases, finding visual variables that are both perceptually accurate (in either estimation of quantity or discrimination of category) as well as perceptually separable from all the other encoding channels, becomes increasingly difficult. 

A further hurdle is that not all visual variables are well-suited for conveying uncertainty. MacEachren \ea~\cite{maceachren2012visual} evaluate a number of visual variables with respect to their \emph{semiotic fit} for representing uncertainty. They observe that certain visual variables such as blurriness and transparency seem to have a more intuitive connection to uncertainty than other variables such as shape or hue. Unfortunately, Boukhelifa \ea~\cite{boukhelifa2012evaluating} find that many visual variables habitually used for conveying uncertainty, such as blur and value, are also difficult to estimate. This results in a preference/performance gap where designers must choose between encoding uncertainty in a way that is intuitive but error prone, or use higher fidelity channels that may not intuitively convey uncertainty.

\subsection{Bivariate Maps}

While uncertainty can be visualized entirely separately from the rest of the data (for instance, in a juxtaposed chart), this runs the risk that uncertainty information becomes \emph{ignorable}~\cite{moritz2017trust}. It also complicates analysis, as interpreting the value and uncertainty of a given data point requires consulting two separate charts. To integrate uncertainty information into an analysis, we focus on the construction of \emph{bivariate maps}, where value and uncertainty information is displayed simultaneously in a spatially co-located manner.

Bivariate maps can, in principle, be constructed from the combination of any two visual variables (such as shape and color, or size and texture). For example Ware's ``textons''~\cite{ware2009quantitative} overlay glyphs on top of regions to simultaneously encode two quantitative values. However, for spatial visualizations like choropleth maps, heatmaps, and treemaps, visual channels such as position and length are reserved for data variables other than value (such as geographic location or relative size). In these situations, data value is often encoded using color. Bivariate maps in these settings therefore typically rely on a visual channel related to color for encoding a secondary variable, such as opacity~\cite{roth2010value} or pixel noise~\cite{lucchesi2017visualizing}.

Colors in univariate quantitative color maps should be sufficiently far apart as to be perceptually distinguishable~\cite{ware1988color}, and vary in lightness as well as hue to afford an implicit ordering of value~\cite{borland2007rainbow,rogowitz2001blair}. Different choices of color maps can highlight different features of the data, and should be chosen with care~\cite{rogowitz1996not}. These principles extend to bivariate color maps, with the additional consideration that our perception of color channels lacks \emph{orthogonality}. That is, while we can perceive e.g., the height of a bar in a bar chart independently of its width, a particular property of a color (e.g., its redness, saturation, transparency, etc.) affect how its other properties are perceived \cite{ware2012information,garner1970integrality}. Many color channels such as hue and saturation are therefore perceptually \emph{integral}, which complicates their use in bivariate maps.

Another recurring design consideration when constructing color maps is whether to quantize data into a discrete set of colors, or to encode data using a continuous mapping. While continuous color maps afford greater fidelity in presenting values~\cite{muller1979perception}, non-linearity in human color perception introduces errors in extracting numeric values from continuous colors~\cite{borland2007rainbow}. Quantizing a color map is therefore an exercise in balancing \emph{perceptual} error and \emph{quantization} error~\cite{dobson1973choropleth}. Discrete maps offer finer control over this balance, which can result in better performance in tasks involving heatmaps~\cite{padilla2017evaluating}.

In general, the quality of bivariate color maps is a multivariate measure involving consideration of not just the component color channels, but also the interpolation scheme and the color of the surround~\cite{bernard2015survey}. However, even simple bivariate maps can be difficult for a general audience to interpret: due to their additional complexity, Wainer \& Francolini~\cite{wainer1980empirical} reported high levels of error for participants even for ``elementary'' graph reading tasks, and additionally that bivariate legends were more difficult to memorize and internalize than their univariate equivalents. Therefore, bivariate maps in practice are often limited to a small set of output colors (say, a 4x4 matrix as in \figref{fig:example}) \cite{robertson1986generation,trumbo1981theory}. For VSUPs, we adapt an insight from Dunn~\cite{dunn1989dynamic} that these limited categories ought to be assigned with regards to their importance to analysis tasks, rather than uniformly.

Given that bivariate maps, for reasons of either practicality or perceptual fidelity, have only a limited budget of outputs, we adapt an insight from Dunn~\cite{dunn1989dynamic} that these limited categories ought to be assigned with regards to their importance to analysis tasks, rather than uniformly. Our Value-Suppressing Uncertainty Palettes apply this insight to data with uncertainty, where, frequently, uncertain information ought to have less valence on the decision-making process, or at least less prominence in the analysis, than certain information.

Correll \ea~\cite{correll2015layercake,correll2011visualizing} use a precursor of VSUPs in their LayerCake genomics visualization tool, where marks representing genomic data with increasing uncertainty are mapped to a smaller and smaller set of increasingly grey colors, creating the effect of uncertain values retreating into a ``confidence fog'' while highly certain values remain prominent. Other bivariate visualizations implicitly alias together uncertain values through perceptual integrality. For instance, if uncertainty is encoded by transparency, a maximally uncertain glyph may be entirely transparent, and so impossible to distinguish from any other maximally uncertain glyph. Other channels with a semiotic connection to uncertainty, such as saturation, value, blur, or size, have similar deleterious effects on the disambiguation of colors and shapes. In both cases, the property of aliasing is \emph{ad hoc}, and places no guarantees on the discriminability of colors. The binning and degradation approach of VSUPs makes the choice to alias values explicit to both the designer and the viewer, and results in a bivariate mapping with known perceptual properties.

\section{Value-Suppressing Uncertainty Palettes}

%TODO need some example figure here, showing tree structure perhaps?
%\flowFig

Value-Suppressing Uncertainty Palettes (VSUPs) are a technique for creating bivariate maps of data \emph{value} and \emph{uncertainty}. VSUPs make two central assumptions about bivariate maps:

\begin{enumerate}
	\item There is a limited \emph{budget} for perceptual discriminability.
	\item Differences among \emph{certain} data are more germane than differences among \emph{uncertain} data.
\end{enumerate}

In some cases, these assumptions are violated. For instance, an analyst might be interested in ``long tail risks'' or other ``black swan events,'' where the impact of a value, no matter how uncertain, must be considered and planned for~\cite{taleb2011black}. Other analysis tasks (such as filtering out outliers), require increased, rather than decreased, discriminability when uncertainty is high. However, for many information fusion tasks, the assumption is that uncertainty is related to data quality, or the uncertainty of data values~\cite{riveiro2007evaluation}.

However, if both of these assumptions hold, then it follows that the designer of a bivariate map should allocate more mark types to certain values, and fewer mark types to uncertain values. VSUPs codify this decision by \emph{reducing the number of mark categories for representing value as uncertainty increases.} This means that data encoded using a VSUP will make visible only the largest of differences in uncertain data, but highlight comparatively small differences in value when uncertainty is low. VSUPs therefore act as both a filtering mechanism (in that values with too much uncertainty are mapped to the same glyph), as well as an implicit test of effect size (in that smaller and smaller changes in value are visible as uncertainty decreases). This strategy of dampening low quality or highly uncertain values in maps in order to focus on more informative regions has measurable benefits, including the removal of statistically spurious visual patterns, and the highlighting of regions of interest~\cite{correll2017surprise}.

VSUPs rely on an underlying \textbf{quantization tree} that governs how values are discretized. Maximally uncertain values are mapped to a singular ``root'' node. As uncertainty decreases, the tree branches into leaves, which evenly divide the data domain. The data value of a parent is the midpoint of all of its children. These leaves can then branch again, up to a designer-specified stopping point. The layers quantize the uncertainty domain, and sibling nodes quantize the data domain. \figref{fig:example} shows a VSUP with a tree with a branching factor of 2 and 4 layers, resulting in an output range with 15 distinct colors.

Since the visual channel used to encode uncertainty (e.g., saturation, lightness, transparency) often reduces the ability of people to distinguish colors, VSUPs have the added benefit of creating bivariate palettes that are perceptually easier to distinguish, by reducing color resolution in precisely the areas where color disambiguation would be most difficult anyway. For instance, the two closest colors in the VSUP in \figref{fig:example} are 1.4 units farther apart in CIELAB color space.

JavaScript code for generating VSUPs for use in D3 charts is available at \url{URL-REMOVED-FOR-REVIEW}.

\subsection{Design Considerations}

There are multiple choices that designers must make before creating a VSUP. Specifically, they must choose (1) which visual channels map to value (1), uncertainty (2), and the structure of the underlying VSUP tree (3).

When selecting a visual channel to represent uncertainty, we recommend channels with both a strong semiotic connection to uncertainty (such as those recommended by MacEachren et al.~\cite{maceachren2012visual}) as well as a relatively large number of perceptually distinguishable levels (such as those evaluated by Boukhelifa et al. \cite{boukhelifa2012evaluating}). Here, we focus on a combination of increasing color value and decreasing saturation. Correll \& Gleicher~\cite{correll2013error} provide evidence that even audiences without statistical training can successfully interpret uncertainty information encoded in these channels.

When selecting a visual channel to represent value, the designer should consider the perceptual integrality of the value and uncertainty visual channels. For instance, we use the Viridis~\cite{viridis} color map frequently in this paper, as many other standard color ramps (such as sequential ColorBrewer~\cite{harrower2003colorbrewer} ramps), following usual best practices, interpolate in both hue and value~\cite{ware1988color}. This interpolation in value interferes with our uncertainty encoding, introducing ambiguity. Ramps with similar saturation and value among colors, such as Virids, are less ambiguous.

When determining the quantization tree, the design should be careful to avoid having too many output colors. For reasons of memorability and complexity, Wainer\& Francolini~\cite{wainer1980empirical} suggest no more than 16. For a binary quantization tree, this results in a maximum tree depth of 4, as seen in \figref{fig:example}. In general, for tasks that require finer-grade distinctions between uncertainty, the depth of the tree should be increased. For tasks involving finer-grade distinctions between value, the branching factor of the tree should be increased. Beyond these two parameters, non-uniform binning would allow the designer to target particular distributions or important subregions of the data that would otherwise be difficult to visualize in a limited palette, as in Dunn's~\cite{dunn1989dynamic} dynamic bivariate maps. As an additional form of control, prior tools using VSUP-like techniques, such as LayerCake~\cite{correll2015layercake}, allow the analyst to interactively control the size of bins in the tree through dragging on the color legend.

\section{Examples}

We present a set of examples showing how VSUPs can be applied to realworld datasets. The decision of VSUPs to allocate color bins asymmetrically amongst value and uncertainty regions affords greater discriminability in regions of the chart that warrant closer scrutiny, while discouraging exploration of regions with noisy or unreliable signals.


\subsection{Air Travel Delay}

\airlineFig

Using a public data set from the U.S. Bureau of Transportation Statistics~\cite{bts}, we created a heatmap showing average flight delay for different times of the day and days of the week for U.S. carriers in January 2017. We use standard error $\left(\sigma / \sqrt{n}\right)$ as our measure of uncertainty. We created two alternate heatmaps of this delay information. \figref{fig:airline2d} shows a traditional bivariate map, whereas \figref{fig:airlineVsum} shows a VSUP generated under the same constraints.

Both maps illustrate a similar temporal trend: flight delays are shorter towards the beginning of the day, increasing on average over time. The traditional bivariate map affords only a coarse examination of this trend: two visible ``blocks'' of color, early in the day, and later in the afternoon. The VSUP, in contrast, has sufficient color resolution to show a quasi-linear trend. Uncertainty is high only for a few bins for which there are small number of flights, either on the weekends or late at night. The VSUP does not obscure any significant trends in the high uncertainty data, despite having fewer color categories overall.

\subsection{Viral Mutation}
%TODO update this figure
\viralFig

Using a dataset from a group of virologists interested in the population dynamics of viral mutation~\cite{o2012conditional}, we visualized the variability in viral genomes across 25 populations of the simian immunodeficiency virus (SIV). Next generation sequencing (NGS) allows biologists to collect and analyze large amounts of genomics data. However, these techniques often create data quality issues, as they must align large numbers of potentially ambiguous ``reads'' of relatively small numbers of base pairs. In the dataset here, it is important not only to identify hotspots in the viral genome (locations with high rates of mutation or variability), but to not be distracted by false positives. The traditional bivariate map (\figref{fig:viral2d}) affords the analysis of only a few, large hot spots. Extending the ``confidence fog'' metaphor used in viral genome browsers like LayerCake~\cite{correll2015layercake}, VSUPs (\figref{fig:viralVsum}) enable biologists to explore interesting patterns while not being distracted by noise. Smaller scale ``hot spots'' that are mapped to the same magenta color in the traditional map are visible as bright orange lines. Pink hot spots towards the end of the genome (around reference nucleotide $9,500$) that could be plausibly considered potential regions of interest in the 2D map, are encoded as highly uncertain and of ambiguous value in the VSUPs. In fact, these regions have systematically low coverage; the lack of reliable data for these regions should discourage analysts from giving their apparent pattern equal consideration from the  stronger signals earlier in the genome.

\textbf{A few thoughts about this section based on the current figures. (1) Fig. 3 in particular has two graphics that look pretty similar. Is there anything we should do to draw attention to the most salient differences between conditions? At minimum, we have the text, but we could consider annotations or even a third vis that is a heatmap of differences between charts. (2) A third example using a different visual form might be useful. For example, a node-link network with value + uncertainty associated with each node.}

\section{Evaluation}
%TODO update conditions fig
\conditionFig

We performed two crowdsourced experiments on Amazon's Mechanical Turk to evaluate the effectiveness of VSUPs for integrating uncertainty and value information in visualizations. This focus on integration meant that we limited our experimental tasks to scenarios where the participants needed to consider both value and uncertainty before making a decision. The experiments consisted of two separate tasks:

\begin{enumerate}
	\item An experiment with an \textbf{identification} task, where we gave participants charts with value and uncertainty information, and asked them to locate specific regions. E.g., ``click on the region of the chart with a value of $0.1$ and an uncertainty of $0.2$.''
	\item An experiment with a \textbf{prediction} task, inspired by the game Battleship, where we gave participants charts with both \emph{forecast} and \emph{forecast uncertainty} information, and asked the participants to place tokens on the board in order to optimize expected value. E.g., ``place your $5$ ships on safe locations on the board.''
\end{enumerate}

We limited our population to Turkers from the United States, with a prior task approval rate of at least 90\%. As the experimental tasks required multi-hue color perception, we presented participants with a set of Ishihara plates~\cite{hardy1945tests} as a pretest, and excluded participants who either misidentified the values in the plates, or who self-reported as having a color vision deficiency (CVD) in the post-test. Based on piloting, we paid participants \$2 dollars for participation, for a target rate of \$8/hour. After completion of the main tasks, we solicited demographic information, including a risk aversion assay from Mandrik \& Bao~\cite{mandrik2005exploring}. We recruited a total of $48$ participants for this experiment ($24$ for each experiment): $24$ female, $24$ male, ($M_{\text{age}}$= $34$, $SD_{\text{age}}$ = $9.1$). Participants from the first experiment were excluded from participating in the second.

Rather than standard means, Cleveland \& McGill~\cite{cleveland1984graphical} use 25\% trimmed means to report effect sizes in graphical perception tasks. Trimmed means remove the first and last quartiles and then compute the mean from the remaining points. This filtering is important for graphical perception tasks (especially crowdsourced tasks) where error distributions may be long-tailed~\cite{heer2010crowdsourcing}. Trimmed means violate the sampling assumptions required for many null-hypothesis significance tests. Thus, while we report effect sizes in terms of trimmed means, we perform inferential statistical tests on the standard means. Asterisks in our charts denote significant differences, as reported by a post-hoc Tukey's test of Honest Significant Difference (HSD).

Experimental materials, including data tables and stimuli generation code, are available at \url{URL-REMOVED-FOR-REVIEW}.


\subsection{Identification Experiment}

We wished to determine what effects, if any, our various design considerations had on performance at a basic analysis task: using a bivariate legend to interpret a heatmap. There are a number of design decisions to be made even after one has determined which two visual channels will be used to encode value and uncertainty.
In particular:
\begin{enumerate}
	\item Whether or not to \emph{juxtapose} or \emph{superimpose} the data and uncertainty maps.
	\item Whether or not to \emph{discretize} each visual channel, or encode values \emph{continuously}.
	\item Whether or not to use a \emph{square} legend, which gives equal area to each bin, or a \emph{wedge} legend, which reduces in size as uncertainty increases.
	\item If discretizing, whether to employ a \emph{VSUP} or a \emph{standard} quantization scheme.
\end{enumerate}

For the identification task, we gave participants a $5$x$5$ heatmap, and asked them to click on a region of the heatmap that encodes a particular value,uncertainty pair. To support comparability across conditions, we chose a set of target pairs such that each target mapped to a unique color across all encoding types. This resulted in (4+4+2+1)=11 valid targets. For each trial, these 11 targets were placed randomly in a heatmap; the remaining 13 cells of the heatmap were distractors randomly sampled from this target list.

We selected this task both as a way of assessing the ability of people to successfully encode and decode values using a bivariate map, and to provide training for the prediction task (since we re-used the bivariate maps in that task). We measured performance both in terms of accuracy (did the participant select the correct point) as well as response time.

Each of the 24 participants saw the data encoded as each of 8 types of bivariate encoding and legend, with 8 replications, for a total of 64 stimuli. \figref{fig:conditions} shows these factor levels.

\subsubsection{Hypotheses}

We had two initial hypotheses, based on prior work on bivariate maps:
\begin{enumerate}
	\item Juxtaposed maps, by introducing a second search task to the identification task (searching for the proper values in two, rather than one map), would have poorer performance than the other conditions.
	\item Continuous maps required more specific and accurate color matching between value and color, and so would have poorer performance than the other conditions.
\end{enumerate}

We had no strong hypotheses about the shape of the legend (square vs. wedge), but, as the wedge shape makes more uncertain bins in the legend smaller (and thus potentially harder to disambiguate), we included it as a check against VSUPs which explicitly alias uncertain values. For this identification task, which did not involve reasoning with uncertainty, but simply matching colors to a color legend, we also did not have strong hypotheses about performance differences between VSUPs and traditional discrete bivariate maps.

\subsubsection{Results}
\taskOneFig

We performed a repeated measures ANOVA on our results to measure the effect of juxtaposed versus superimposed maps, and continuous versus discrete legends, on accuracy. Participant ID was included as a random effect. \figref{fig:taskOne} shows the performance of each of our 8 conditions.

Our results support our first hypothesis. Juxtaposition was a significant effect ($F(1,166)=5.5$,$p=0.02$). A post-hoc t-test confirmed ($t=2.5$, $p=0.01$)that superimposed charts ($M=0.58$, $SD=0.5$) performed significantly better than juxtaposed ($M=0.51$, $SD=0.49$) ones. For our identification task, juxtaposition requires at least two search tasks (one to locate squares with the correct data value, another to locate ones with the correct uncertainty value), which introduces additional error compared to conditions where value and uncertainty are displayed simultaneously.

Our results also support our second hypothesis. Discretization was a significant effect ($F(1,166)=30$,$p<0.01$). A post-hoc t-test confirmed ($t=6.1$, $p<0.01$) that charts with discrete bins ($M=0.63$, $SD=0.48$) performed significantly better than charts with continuous color maps ($M=0.47$, $SD=0.5$). The lack of quantization bias in continuous maps is countered by the perceptual error in precisely estimating value from color. Relying on a discrete set of output colors simplifies this task.

We performed a second ANOVA amongst just the superimposed, discrete charts, to determine the effect of legend shape (wedge or square) and quantization scheme (VSUP or standard) on performance, with participant ID as a random factor. Neither the legend shape ($F(1,70)=0.03$, $p=0.084$) nor the quantization scheme ($F(1,70)=1.4$, $p=0.24$) were significant effects.

\subsection{Prediction Experiment}

\taskTwoFig

For the prediction task, we gave participants the rules of a game like Battleship. Greis et al. \ea~\cite{greis2016decision} employ these game-like experimental tasks to assess how different visual designs communicate uncertainty information, which can be abstract or complex, to the general audience. In our task, the participant and a (fictional) adversary have to place tokens representing ships on a $5$x$5$ spatial grid, with the expectation that certain squares will be hit by missiles. Players have to place all their tokens before continuing. The objective is to minimize the number of your own ships that are hit. In our task, participants were given a map representing the predictions of missile strikes in each location on the grid. The \emph{value} component of the prediction was the likelihood of a missile strike in a particular region: the ship's \emph{safety} if placed on the square. The \emph{uncertainty} component was the confidence in this prediction. Other studies of uncertainty representation, such as in Cox et al.~\cite{cox2013visualizing}, have used "prediction + prediction uncertainty" stimuli to elicit differences in decision-making between visualizations of uncertainty.

Our stimuli were created by randomly sampling from values that fell within each of the $16$ bins of the $4$x$4$ 2D bivariate color map. This resulted in 16 samples. The remaining 9 samples were distractors, with low safety and high certainty.

We selected this task in order to promote risk-averse behavior. Tversky \& Kahneman~\cite{tversky1985framing} illustrate that framings in terms of gains or losses produce reliably different outcomes. In particular, there is greater perceived value in avoiding large losses as opposed to striving for a large gain~\cite{kahneman1979prospect}. 

The ideal strategy from a value-maximizing standpoint would be to place tokens on areas with the highest expected value (lowest danger), ignoring the uncertainty information. However, as with roulette and other similar games of chance, the \emph{variability} in expected value is relevant when considering where to place bets~\cite{mlodinow2009drunkard}. A risky player would choose guesses with high expected value, regardless of the uncertainty of those points. A more conservative guesser might eschew high-risk, high-reward locations, resulting in a lower average value of guesses, but also lower uncertainty. We therefore measured the distribution of both \emph{value} and \emph{uncertainty} of the tokens placed by the participants.

Our results from the prior study indicate indicate that discrete, non-juxtaposed maps outperformed the other bivariate maps we selected, so we limited our study to only 4 types: square and arc bivariate maps. These in turn were either VSUPs or traditional bivariate maps, for a 2 (square or arc legend) x 2 (VSUP or standard) factorial design, with 6 replications, for a total of 24 stimuli for each of our 24 participants. \figref{fig:conditions} shows these factor levels. Prior to the main task, we included a short replication of the identification task from the prior experiment (with 12 stimuli) for training purposes. Poor performance on this training was used for exclusion purposes. Three people with low accuracy ($6\%$, $6\%$, and $25\%$ accuracy compared to a mean of $70\%$ ) were excluded. 

\subsubsection{Hypotheses}

We had two hypotheses, stemming from our belief that VSUPs promote better \emph{integration} between uncertainty and value information, and also encourage \emph{caution} by highlighting the ambiguity or untrustworthiness introduced by uncertain data. In particular:
\begin{enumerate}
	\item Participants would choose targets with \textbf{lower uncertainty} in the prediction task when using a VSUP. 
	\item This would result in a tradeoff where they would also choose targets with \textbf{lower expected value} when using a VSUP.
\end{enumerate}

As with the prior experiment, we had no strong hypotheses for square vs. wedge-shaped legends, but included both as a check against the potential implicit VSUP-like properties of wedge-shaped legends.

\subsubsection{Results} 
\taskTwoViolin
Consistent with our first experiment, we found no significant effect of legend shape on either uncertainty ($F(1,61) = 0.01$, $p=0.92$) or value ($(1,61)=3.1$, $p=0.08$) of guesses. This seems to indicate that both wedge- or square-shaped legends promote similar patterns of decision-making. In this paper, and in general, we prefer to employ the wedge-shaped legend for VSUPs, and the square-shaped legend for traditional maps, as it makes the conceptual differences between the two more apparent.

Our results failed to support our first hypothesis. We performed a repeated measures ANOVA on our results to measure the effect of VSUP versus standard quantizations, and square- versus wedge-shaped legends, on average uncertainty in bets. We found no significant effect of quantization scheme on average guess uncertainty ($F(1,61)=0.05$, $p=0.83$). This indicates that there does not appear to be a uniform pattern of risk aversion between the two scale types, as we had surmised.

Our results supported our second hypothesis. A similar ANOVA on average danger value of guesses found that quantization scheme was a significant effect ($F(1,61)=17$, $p<0.01$). A post-hoc t-test confirmed ($t=2.3$, $p=0.02$) that guesses made with VSUPs had significantly higher average danger ($M=0.32$, $SD=0.17$) (and so lower expected value) than traditional maps ($M=0.29$, $SD=0.18$). Since there was no significant difference in average guess uncertainty, and the distribution of danger and uncertainty was identical across conditions, this seems to indicate that participants were using VSUPs moderating their guesses: avoiding highly uncertain regions, but willing to ``gamble'' more on regions of middling certainty. \figref{fig:taskTwoViolin} shows the comparative distribution of uncertainty in participant guesses.

\section{Discussion}

\sizeFig

The results from our initial evaluation of VSUPs are promising, showing that even a general audience can make use of uncertainty information in a reasonable way. The way that uncertainty information is presented can have a measurable impact on decision-making. The VSUP strategy of only ``showing'' differences when uncertainty is sufficiently low is somewhat analogous to the inferential statistics such as effects tests. VSUPs might therefore be used to promote more caution in judgments, and lead analysts away from spurious signals in data.

Bivariate maps have an inherent limitation in that visual channels are often difficult to attend to separately or orthogonally. Bivariate maps are also complex to interpret. Systems which simultaneously display large amounts of value and uncertainty data to general audiences (such as Pangloss~\cite{moritz2017trust}) avoid them for precisely these reasons, relying instead on juxtaposed maps. Our results indicate that juxtaposition produces additional problems, forcing users that wish to integrate uncertainty and data to locate an item of interest, and then re-locate that same item in a visually distant map, which may have few visual landmarks in common. This additional step before information fusion is reflected in longer response times, and patterns of decision-making that are less mindful of risk. Given the deleterious effects of latency on data analysis~\cite{liu2014effects}, designers ought to carefully consider the tradeoffs between expressiveness of visual encoding, and usability.

\subsection{Limitations and Future Work}

VSUPs assume that uncertain values ought to have less visual impact that highly certain values. VSUPs therefore act both as a kind of filtering device as well as a bivariate representation. This assumption holds in scenarios like the virology dataset (\figref{fig:viralVsum}), where the expectation is that highly uncertain data is unreliable or should otherwise be downweighted in analysis. If the analyst has a different interpretation of uncertainty, or wishes to quickly and orthogonally analyze the distributions of uncertainty and value, other strategies, such as juxtaposed maps, may be more appropriate. VSUPs are designed for the \emph{integration} of value and uncertainty. Designers should take care in considering when and how this integration is desirable.

Our experiments dealt with cases where both uncertainty and value were represented by color. The perceptual non-separability of color channels is well-known~\cite{garner1970integrality, ware2012information}, and so the concept of a limited ``budget'' of distinguishable marks easier to quantify and illustrate. In principle, a VSUP can be created for any combination of visual variables. All that is required is a perceptual model of the interaction between these two variables. Where these models exist, as with the interaction between size and color~\cite{stone2014engineering}, the creation of VSUPs is straightforward. Where these models do not exist, or where the perceptual interaction is too complex to efficiently model, experimental work remains to be done before VSUPs can be considered a feasible design strategy. We are currently experimenting with the creation of VSUPs using these other channels, as in \figref{fig:size}.

\subsection{Conclusion}

Often, uncertainty, data quality, or confidence are considered separate information from the data itself, relegated to tooltips or visually distant supplemental charts. We believe, in contrast, that uncertainty information ought to be directly integrated with charts of data itself. This integration introduces additional complexity in the design and presentation of data. Value-Suppressing Uncertainty Palettes represent one strategy for dealing with this complexity, by assigning marks properties in a way that supports the disambiguation of values in data where uncertainty is low, but suppresses these judgments when uncertainty is high. This decision of how to allocate visual variables promotes patterns of decision-making that make responsible use of uncertainty information, discouraging comparison of values in unreliable regions of the data, and promoting comparison in regions of high certainty.